# Leveraging static analysis for evaluating code-generation models

This repository contains code-base for CSCI 544 course project titled "Leveraging static analysis for evaluating code-generation models", by Group 37.<br>
Here are some relevant links:<br>
[Paper presentation](https://github.com/ksanu1998/NLP_Group37/blob/main/reports/NLP_Group_37_Paper_Presentation.pdf)<br>
[Project proposal](https://github.com/ksanu1998/NLP_Group37/blob/main/reports/NLP_Group_37_Project_Proposal.pdf) <br>
[Project status report](https://github.com/ksanu1998/NLP_Group37/blob/main/reports/NLP_Group_37_Project_Status_Report.pdf) <br>
[Project presentation](https://github.com/ksanu1998/NLP_Group37/blob/main/reports/NLP_Group_37_Project_Presentation.pdf) <br>
Project final report <br>

Abstract:<br>
Code generation has recently become a popular application of Large Language Models (LLMs). ChatGPT \cite{chatgpt}, \href{https://github.com/features/copilot}{GitHub CoPilot}, Code Llama \cite{roziere2023code}, \href{https://blog.google/technology/ai/bard-google-ai-search-updates/}{Bard} are some popular tools that aim to enhance developer productivity and shorten development time. Often, code generated by these tools remains buggy, rendering them counterproductive. Existing methods that address this issue predominantly focus on runtime analysis, which is resource-intensive, while limited research has been conducted in the realm of static analysis, particularly for a narrow selection of programming languages. We aim to impart knowledge of static errors to the baseline code generation model, which potentially helps in improving code generation. Towards this goal, we propose a pipeline that integrates feedback obtained from static analysis to the baseline model. Moreover, we fine-tune the baseline model using the samples rejected due to static errors. Our findings demonstrate the effectiveness of both strategies regarding reducing frequencies of observed static errors.

TODO:<br>
* Format the README nicely
* Explain setup guidlines for linters and LLMs (if needed)
* Explain all folders with link. Make sure to mention that the post_feedback is a proof of concept of automation. 
* Results in results/post_feedback are to demonstrate the automation and not the results of actual feedback analysis.
* Write something about LLMs that Abhishek is going to merge.
* Add author names and github handles
* Add link to final report
